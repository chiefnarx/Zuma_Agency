# ğŸ  Zuma Real Estate Data Pipeline

This project delivers a full data engineering workflow for property listings and metadata extracted from raw real estate datasets via API. Built using PostgreSQL and Python, it handles schema creation, data cleaning, dimensional modeling, and robust ETL inserts, while managing complex data types and database constraints.

---

## ğŸ“Œ Project Overview

This project builds and populates the Zuma Agency database from scratch, covering:

âœ… PostgreSQL schema design and table creation  
âœ… Data transformation and cleaning with pandas  
âœ… Dimensional model: facts + dimensions  
âœ… Automated data loading with Python

---

## âš™ï¸ Technologies Used

- âº PostgreSQL: Relational Database Design  
- âº Python (`pandas`, `json`): Data Cleaning and ETL  
- âº SQL (`DDL`, `DML`): Schema and Insert Statements

---

## ğŸ”„ Full ETL Workflow

### 1. Data Preparation and Cleaning

- Loaded raw real estate data using pandas  
- Cleaned and casted columns like `bedrooms`, `bathrooms`, `squareFootage`  
- Serialized JSON fields (e.g. `features`) using `json.dumps()`  
- Removed outliers to prevent type mismatches during inserts  
- Standardized null and invalid entries

---

### 2. Dimensional Model Creation

Created the following tables:
- `location_dim`  
- `features_dim`  
- `owner_dim`  
- `property_fact`

---

### 3. Insert Logic and Error Handling

- Used SQL `INSERT INTO` statements with placeholders and cursor execution  
- Casted boolean values to fix PostgreSQL mismatches  
- Implemented `ON CONFLICT DO NOTHING` to prevent insert duplication  
- Debugged numerous runtime errors: type mismatch, column count mismatch, datatype coercion, etc.

---

## ğŸ“„ Results & Final Verification

âœ… Cleaned and structured real estate dataset  
âœ… All dimensional tables populated without conflict  
âœ… Fact table normalized and aligned to schema  
âœ… PostgreSQL queries return joined data across dimensions  
âœ… Database schema scalable for analytics and future integrations

---

## ğŸ“ Notes

 âº Used `SERIAL` IDs for automatic dimension key generation  
 âº Foreign key IDs (`features_id`, `location_id`, and `owner_id`) were automatically matched based on the dimension table values  
 âº Used Python to format data as proper `True/False` before inserting into the database
