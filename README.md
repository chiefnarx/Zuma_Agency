ğŸ  **Zuma Real Estate Agency - Data Pipeline**

This project delivers a full data engineering workflow for property listings and metadata extracted from raw real estate datasets. Built using PostgreSQL and Python, it handles schema creation, data cleaning, dimensional modeling, and robust ETL inserts, while managing complex data types and database constraints.

ğŸ“Œ **Project Overview**

  This project builds and populates the Zuma Agency database from scratch, covering:

    âœ… PostgreSQL schema design and table creation
  
    âœ… Data transformation and cleaning with pandas
  
    âœ… Dimensional model: facts + dimensions
  
    âœ… Automated data loading with Python

âš™ï¸ **Technologies Used**

    âº PostgreSQL: Relational Database Design 

    âº Python (pandas, json): Data Cleaning and ETL 

    âº SQL (DDL, DML): Schema and Insert Statements 

ğŸ”„ **Full ETL Workflow**

  1. Data Preparation and Cleaning:
     
    â€¢ Loaded raw real estate data using pandas
    â€¢ Cleaned and casted columns like bedrooms, bathrooms, squareFootage
    â€¢ Serialized JSON fields (e.g. features) using json.dumps()
    â€¢ Removed outliers to prevent type mismatches during inserts â€¢ Standardized null and invalid entries

  2. Dimensional Model Creation:

    Created the following tables:
    â€¢ location_dim
    â€¢ features_dim
    â€¢ owner_dim
    â€¢ property_fact

  3. Insert Logic and Error Handling:

    â€¢ Used SQL INSERT INTO statements with placeholder and cursor execution.
    â€¢ Casted boolean values to fix PostgreSQL mismatches.
    â€¢ Implemented ON CONFLICT DO NOTHING to prevent insert duplication
    â€¢ Debugged numerous runtime errors: type mismatch, column count mismatch, datatype coercion, etc.

  ğŸ“„ **Results & Final Verification**

    âœ… Cleaned and structured real estate dataset.
  
    âœ… All dimensional tables populated without conflict.
  
    âœ… Fact table normalized and aligned to schema.
  
    âœ… PostgreSQL queries return joined data across dimensions.

    âœ… Database schema scalable for analytics and future integrations.

  ğŸ“ Notes

    âº Used SERIAL IDs for automatic dimension key generation

    âº Foreign key IDs (features_id, location_id, and owner_id) were automatically matched based on the dimension table values

    âº Used Python to format data as proper True/False before inserting into the database
